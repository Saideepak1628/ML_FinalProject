{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Reviews.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(data.info())\n",
    "\n",
    "# Display the first few rows of the \"Text\" column to understand what kind of data we're dealing with\n",
    "print(data['Text'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Datset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing HTML tages, converting to lowercase, tokenize text and Removing special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to clean and tokenize text, then convert back to a cleaned string\n",
    "def clean_and_tokenize_to_string(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Convert to lower case\n",
    "    text = text.lower().strip()\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove special characters and digits from each token\n",
    "    tokens = [re.sub(r'[^a-zA-Z\\s]', '', token) for token in tokens]\n",
    "    # Remove empty tokens\n",
    "    tokens = [token for token in tokens if token]\n",
    "    # Join tokens back into a single string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "data['clean_text'] = data['Text'].apply(clean_and_tokenize_to_string)\n",
    "\n",
    "# Display the first few entries of the cleaned text\n",
    "print(data['clean_text'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Assuming data['clean_text'] is loaded\n",
    "data['no_stopwords'] = data['clean_text'].apply(remove_stopwords)\n",
    "\n",
    "# Display the first few entries without stopwords\n",
    "print(data['no_stopwords'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to apply stemming\n",
    "def stem_text(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Assuming data['no_stopwords'] is loaded\n",
    "data['stemmed_text'] = data['no_stopwords'].apply(stem_text)\n",
    "\n",
    "# Display the first few entries of the stemmed text\n",
    "print(data['stemmed_text'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying first 5 Rows after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Dataset and Apply Vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "data['Sentiment'] = data['Score'].apply(lambda x: 1 if x > 3 else 0)\n",
    "\n",
    "# Assuming 'stemmed_text' is already in your dataset from the preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['stemmed_text'], data['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and apply TF-IDF Vectorizer with n-grams\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))  # This includes both unigrams and bigrams\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm_model = make_pipeline(StandardScaler(with_mean=False), SGDClassifier(loss='hinge'))\n",
    "\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_svm = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy of the SVM model: {accuracy_svm}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation for 5 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "cv_results = cross_validate(svm_model, X_train_tfidf, y_train, cv=5, scoring=scoring)\n",
    "df_cv_results = pd.DataFrame(cv_results)\n",
    "print(df_cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting one review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_svm(review):\n",
    "  \n",
    "    review_cleaned = clean_and_tokenize_to_string(review) \n",
    "    review_vectorized = vectorizer.transform([review_cleaned])\n",
    "    prediction = svm_model.predict(review_vectorized)\n",
    "    return 'Positive' if prediction[0] == 1 else 'Negative'\n",
    "\n",
    "# Example usage\n",
    "new_review = \"This product was great! Very helpful and works perfectly.\"\n",
    "print(predict_sentiment_svm(new_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Metrics Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_scores = svm_model.decision_function(X_test_tfidf)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC-PR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "auc_pr = auc(recall, precision)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve (area = %0.2f)' % auc_pr)\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='blue')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'sgdclassifier__alpha': [0.0001, 0.001, 0.01],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svm_model, param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "important_columns = ['param_sgdclassifier__alpha', 'mean_test_score', 'std_test_score']\n",
    "print(df_results[important_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of the SVM model: {accuracy}')\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_scores = cross_val_score(svm_model, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "mean_cv_score = cross_val_scores.mean()\n",
    "std_cv_score = cross_val_scores.std()\n",
    "\n",
    "print(f\"Mean cross-validation accuracy: {mean_cv_score:.4f}\")\n",
    "print(f\"Standard deviation of cross-validation accuracy: {std_cv_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_to_rating(prob_positive):\n",
    "    return round(1 + 4 * prob_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sentiment_to_stars(prob_positive):\n",
    "    if prob_positive > 0.9:\n",
    "        return 5\n",
    "    elif prob_positive > 0.75:\n",
    "        return 4\n",
    "    elif prob_positive > 0.55:\n",
    "        return 3\n",
    "    elif prob_positive > 0.3:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "svm_pipeline = make_pipeline(StandardScaler(with_mean=False), SGDClassifier(loss='hinge'))\n",
    "\n",
    "calibrated_clf = CalibratedClassifierCV(svm_pipeline, method='sigmoid', cv=5)\n",
    "\n",
    "calibrated_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_prob = calibrated_clf.predict_proba(X_test_tfidf)\n",
    "print(\"Predicted probabilities:\\n\", y_prob)\n",
    "\n",
    "y_pred = calibrated_clf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of the SVM model: {accuracy}')\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting rating from 1 to 5 for text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, probs in enumerate(y_prob):\n",
    "    positive_prob = probs[1]\n",
    "    star_rating = map_sentiment_to_stars(positive_prob)\n",
    "    print(f\"Review: {X_test.iloc[i]}\")\n",
    "    print(f\"Star Rating: {star_rating} stars\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_tfidf = vectorizer.transform(data['stemmed_text']) \n",
    "y_prob_all = calibrated_clf.predict_proba(data_tfidf)\n",
    "\n",
    "data['Star Rating'] = [map_sentiment_to_stars(prob[1]) for prob in y_prob_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying text and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[['stemmed_text', 'Star Rating']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Rating vs reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.countplot(x='Star Rating', data=data, palette='viridis') \n",
    "\n",
    "plt.title('Distribution of Star Ratings')\n",
    "plt.xlabel('Star Ratings')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
